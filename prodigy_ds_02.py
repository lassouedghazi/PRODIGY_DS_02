# -*- coding: utf-8 -*-
"""PRODIGY_DS_02

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15y2qYxz2tV6zjP18FUFV1Lv-SEWTPLeu
"""

'''
For exploratory data analysis (EDA) and trend analysis, you should use the train set.

Here's why:

Train Set: The train set is used to explore the data, understand the relationships between variables,
and identify trends. This helps in feature engineering, understanding distributions, and deciding which models to apply.

Test Set: The test set is typically kept aside and used only for evaluating the performance of your model
after it has been trained. It should remain untouched during EDA to ensure an unbiased evaluation
of your model's performance.

By using only the train set for EDA, you avoid "leaking" information from the test set into your analysis,
which could lead to overfitting and unreliable model performance when you finally test it.
'''

import pandas as pd
import numpy as np
data=pd.read_csv("/content/tested.csv")

data

data.info()

'''Hereâ€™s a breakdown of the columns in your dataset:

1. **PassengerId**: A unique identifier for each passenger.

2. **Survived**: Indicates whether the passenger survived (1) or did not survive (0).

3. **Pclass**: The passenger's class on the Titanic (1st = 1, 2nd = 2, 3rd = 3).
This is a proxy for socioeconomic status (1st class = upper, 2nd class = middle, 3rd class = lower).

4. **Name**: The full name of the passenger, which may include titles and other details.

5. **Sex**: The gender of the passenger (male or female).

6. **Age**: The age of the passenger in years. Fractional values may represent children under one year old.

7. **SibSp**: The number of siblings or spouses the passenger had aboard the Titanic.
This includes brothers, sisters, stepbrothers, stepsisters, and spouses.

8. **Parch**: The number of parents or children the passenger had aboard the Titanic.
This includes mothers, fathers, daughters, sons, stepdaughters, and stepsons.

9. **Ticket**: The ticket number of the passenger.

10. **Fare**: The fare paid for the ticket, in British pounds.

11. **Cabin**: The cabin number assigned to the passenger. Many entries may be missing,
as not all passengers were assigned cabins.

12. **Embarked**: The port of embarkation where the passenger boarded the Titanic:
   - **C** = Cherbourg
   - **Q** = Queenstown
   - **S** = Southampton

These columns provide a mix of categorical and numerical data that can be used to explore
 various relationships, trends, and patterns in the Titanic dataset.'''

data.isnull().sum()

''' So we have null values in Age, Fare and Cabin  column'''

''' So first we are going to fill the null values in Age column with the median value of the column,
we did not choose the mean value because it may affect the distribution of the data as we know that the
mean is sensitive to outliers.'''

data['Age'].fillna(data['Age'].median(),inplace=True)

'''now we are going to check if the null values are filled or not'''

data['Age'].isnull().sum()

'''so we have filled the null values in Age column'''

''' now we are going to fill the null values in Fare column with the median value of the column.
We know that the Fare column is a numerical column so we can use the median value to fill the null values.'''

data['Fare'].fillna(data['Fare'].median(),inplace=True)

'''now we are going to check if the null values are filled or not'''

data['Fare'].isnull().sum()

'''so we have filled the null values in Fare column'''

''' lets compute the percentage of null values in Cabin column'''

data['Cabin'].isnull().sum()/len(data)*100

'''now we are going to fill the null values in Cabin column with the mode value of the column.
We know that the Cabin column is a categorical column so we can use the mode value to fill the null values.
But as we know that the Cabin column has a lot of null values so we are going to drop the column.'''

data.drop('Cabin',axis=1,inplace=True)

''' now we are going to check if the column is dropped or not'''

data.columns

'''so we have dropped the column'''

''' now we have completed the null values in the dataset'''

data.head()

''' we are going to drop the Name column because it is not useful for our analysis'''

data.drop('Name',axis=1,inplace=True)

''' we are going to drop the passangerId column because it is not useful for our analysis'''

data.drop('PassengerId',axis=1,inplace=True)

'''also we are going to drop the Ticket column because it is not useful for our analysis'''

data.drop('Ticket',axis=1,inplace=True)

data.head()

''' now we are going to check duplicate values in the dataset'''

data.duplicated().sum()

''' we have 41 duplicate values in the dataset, we need to remove them'''

data.drop_duplicates(inplace=True)

''' lets check if the duplicate values are removed or not'''

data.duplicated().sum()

'''so we have removed the duplicate values'''

'''now lets check the validity of data types'''

data.info()

''' the only column that makes some confusion is the Age column which is a float column , we should convert it to int column'''

data['Age']=data['Age'].astype(int)

data

''' now our data is ready for EDA'''

data.describe()

''' now lets check if there are outliers in the numerical columns using IQR Method'''

Q1 = data.select_dtypes(include=['number']).quantile(0.25)
Q3 = data.select_dtypes(include=['number']).quantile(0.75)
IQR = Q3 - Q1
outliers = ((data.select_dtypes(include=['number']) < (Q1 - 1.5 * IQR)) | (data.select_dtypes(include=['number']) > (Q3 + 1.5 * IQR))).sum()
outliers

''' we are going to remove the outliers using IQR Method'''

Q1 = data.select_dtypes(include=['number']).quantile(0.25)
Q3 = data.select_dtypes(include=['number']).quantile(0.75)
IQR = Q3 - Q1
data_no_outliers = data[~((data.select_dtypes(include=['number']) < (Q1 - 1.5 * IQR)) | (data.select_dtypes(include=['number']) > (Q3 + 1.5 * IQR))).any(axis=1)]

'''now our new data is ready for EDA'''

data_no_outliers

'''now lets do some data visualization'''

''' lets see the percentage of people survived and not survived
in the dataset in the form of a pie chart'''

survived_counts = data['Survived'].value_counts()
import matplotlib.pyplot as plt
plt.pie(survived_counts, labels=['Not Survived', 'Survived'], autopct='%1.1f%%', startangle=90)
plt.title('Percentage of People Survived and Not Survived')
plt.show()

''' we can make it more visually appealing'''

import matplotlib.pyplot as plt

# Custom color palette
colors = ['#ff9999','#66b3ff']

# Create the pie chart
plt.figure(figsize=(8, 8))
plt.pie(survived_counts,
        labels=['Not Survived', 'Survived'],
        autopct='%1.1f%%',
        startangle=90,
        colors=colors,
        shadow=True,
        explode=(0.05, 0.05),  # Slightly separate the slices
        textprops={'fontsize': 14, 'weight': 'bold'},  # Bold and larger font
        wedgeprops={'edgecolor': 'black', 'linewidth': 2})  # Add a border to slices

# Title with a larger, bold font
plt.title('Percentage of People Survived and Not Survived', fontsize=16, weight='bold')

# Display the chart
plt.show()

''' lets see the percentage of male and female
in the dataset in the form of a pie chart'''

gender_counts = data['Sex'].value_counts()
import matplotlib.pyplot as plt
plt.pie(gender_counts, labels=['Male', 'Female'], autopct='%1.1f%%', startangle=90)
plt.title('Percentage of Male and Female Passengers')
plt.show()

''' now let see the percentage of males and females survived
in a pie chart'''

# Filter the data for survived passengers
survived_data = data_no_outliers[data_no_outliers['Survived'] == 1]

# Count the number of survived males and females
survived_males = survived_data[survived_data['Sex'] == 'male'].shape[0]
survived_females = survived_data[survived_data['Sex'] == 'female'].shape[0]

# Create the pie chart
import matplotlib.pyplot as plt
plt.pie([survived_males, survived_females], labels=['Survived Males', 'Survived Females'], autopct='%1.1f%%', startangle=90)
plt.title('Percentage of Survived Males and Survived Females')
plt.show()

# Filter the data for passengers who did not survive
not_survived_data = data_no_outliers[data_no_outliers['Survived'] == 0]

# Count the number of not survived males and females
not_survived_males = not_survived_data[not_survived_data['Sex'] == 'male'].shape[0]
not_survived_females = not_survived_data[not_survived_data['Sex'] == 'female'].shape[0]

# Create the pie chart
import matplotlib.pyplot as plt
plt.pie([not_survived_males, not_survived_females], labels=['Not Survived Males', 'Not Survived Females'], autopct='%1.1f%%', startangle=90)
plt.title('Percentage of Not Survived Males and Not Survived Females')
plt.show()

survived_females_count = data_no_outliers[(data_no_outliers['Survived'] == 1) & (data_no_outliers['Sex'] == 'female')].shape[0]
print(survived_females_count)

female_count = data_no_outliers[data_no_outliers['Sex'] == 'female'].shape[0]
print(female_count)

data_no_outliers.head()

'''now lets do data transformation for categorical data to satrt machine learning'''

''' lets convert the categorical data to numerical data'''

'''lts do one hot encoding for sex column'''

data_no_outliers = pd.get_dummies(data_no_outliers, columns=['Sex'], drop_first=True,dtype=int)

data_no_outliers.head()

''' now one hot encoding for embarked column'''
data_no_outliers = pd.get_dummies(data_no_outliers, columns=['Embarked'], drop_first=True,dtype=int)

data_no_outliers.columns

''' now our data is ready for machine learning'''

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, r2_score, recall_score

# Assuming 'data_no_outliers' is your preprocessed DataFrame
# Extract features (X) and target variable (y)
X = pd.get_dummies(data_no_outliers.drop('Survived', axis=1))
y = data_no_outliers['Survived']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize a RandomForestClassifier
model = RandomForestClassifier()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
r_squared = r2_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("F1 Score:", f1)
print("Recall:", recall)
print("R-squared:", r_squared)

# Print the first few predictions and their corresponding actual values
print("Predicted labels:", y_pred[:10])
print("Actual labels:", y_test.values[:10])

# Check the distribution of the predictions
print("Distribution of predictions:", pd.Series(y_pred).value_counts())

import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Assuming the model has been trained as before
# Example of a new input (replace with actual values corresponding to your features)
new_input = {
    'Pclass': 3,
    'Age': 22,
    'SibSp': 1,
    'Parch': 0,
    'Fare': 7.25,
    'Sex_male': 1,
    'Embarked_Q': 0,
    'Embarked_S': 1
}

# Convert the new input to a DataFrame
new_input_df = pd.DataFrame([new_input])

# Ensure the new input has the same structure as X
new_input_df = new_input_df.reindex(columns=X.columns, fill_value=0)

# Make a prediction
predicted_output = model.predict(new_input_df)

# Interpret the result
survival_status = "Survived" if predicted_output[0] == 1 else "Did not survive"
print("Predicted output:", survival_status)